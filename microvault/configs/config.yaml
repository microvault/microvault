# Twin Delayed DDPG
agent:
  gamma: 0.99 # Factor de desconto
  tau: 1e-3 # Atualização suave de parâmetros de destino
  update_every_step: 2 # Taxa de atualização da rede ator
  lr_actor: 1e-3 # Taxa de aprendizado do ator
  lr_critic: 1e-3 # Taxa de aprendizado do crítico
  noise: 0.2
  noise_std: 0.1
  noise_clip: 0.5
  normal_scalar: 0.25
  nstep: 10
  eta: 0.1
  weight_decay: 0.0

# Noise Layer
noise_layer:
  desired_distance: 0.7
  scalar: 0.05
  scalar_decay: 0.99

# Experience Replay buffer
replay_buffer:
  buffer_size: 1048576 # Tamanho do buffer de repetição
  epsilon: 0.01
  alpha: 0.6
  beta: 0.4
  beta_increment_per_sampling: 1e-4
  absolute_error_upper: 1.0

# Environment
environment:
  timestep: 100 # max step
  size: 2.0
  fps: 50 # 10 frames per second
  random: 100000000000 # 100 random points
  threshold: 0.005 # 0.1 threshold
  grid_lenght: 10 # TODO: error < 5 -> [5 - 15]
  state_size: 13
  action_size: 2

robot:
  min_radius: 1.0
  max_radius: 3.0
  wheel_radius: 0.3
  wheel_base: 0.3
  fov: 6.28
  num_rays: 10
  max_range: 6.0
  max_action: 1.0
  min_action: -1.0
  max_linear: 1.0 # 0.2 m/s
  min_linear: 0.0 # 0.1 m/s
  max_angular: 1.0 # 0.2 rad/s
  min_angular: -1.0 # 0.1 rad/s

# Engine
engine:
  seed: 42
  device: cpu
  batch_size: 128 # Tamanho do lote
  num_episodes: 2000
  pretrained: False
  checkpoint: 200
  path: "./"
  wandb: True
  save_checkpoint: 200
  epochs: 1000 # 1000 epochs
  visualize: False

# Model Network
network:
  layers_actor_l1: 400
  layers_actor_l2: 300
  layers_critic_l1: 400
  layers_critic_l2: 300
  activation: relu
